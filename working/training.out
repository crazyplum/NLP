Using SCRIPTS_ROOTDIR: /Users/plum/Developer/NLP/mosesdecoder/scripts
Using single-thread GIZA
(1) preparing corpus @ Sun Jan  4 18:52:21 CST 2015
Executing: mkdir -p /Users/plum/Developer/NLP/working/test_anc/corpus
(1.0) selecting factors @ Sun Jan  4 18:52:21 CST 2015
(1.1) running mkcls  @ Sun Jan  4 18:52:21 CST 2015
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/mkcls -c50 -n2 -p/Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.mod -V/Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb.classes opt
Executing: /Users/plum/Developer/NLP/working/../mosesdecoder/tools/mkcls -c50 -n2 -p/Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.mod -V/Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 4661

start-costs: MEAN: 173217 (173177-173257)  SIGMA:40.0224   
  end-costs: MEAN: 154837 (154622-155052)  SIGMA:215.308   
   start-pp: MEAN: 531.649 (530.466-532.832)  SIGMA:1.18263   
     end-pp: MEAN: 191.428 (189.137-193.718)  SIGMA:2.29068   
 iterations: MEAN: 107315 (106248-108382)  SIGMA:1067   
       time: MEAN: 2.09042 (2.06888-2.11195)  SIGMA:0.021535   
(1.1) running mkcls  @ Sun Jan  4 18:52:26 CST 2015
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/mkcls -c50 -n2 -p/Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.anc -V/Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb.classes opt
Executing: /Users/plum/Developer/NLP/working/../mosesdecoder/tools/mkcls -c50 -n2 -p/Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.anc -V/Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 4291

start-costs: MEAN: 102342 (102253-102431)  SIGMA:89.1461   
  end-costs: MEAN: 88779.1 (88770.6-88787.6)  SIGMA:8.46928   
   start-pp: MEAN: 393.994 (390.879-397.11)  SIGMA:3.11561   
     end-pp: MEAN: 118.294 (118.205-118.383)  SIGMA:0.0888729   
 iterations: MEAN: 97253.5 (96009-98498)  SIGMA:1244.5   
       time: MEAN: 1.63415 (1.6011-1.66721)  SIGMA:0.033056   
(1.2) creating vcb file /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb @ Sun Jan  4 18:52:29 CST 2015
(1.2) creating vcb file /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb @ Sun Jan  4 18:52:29 CST 2015
(1.3) numberizing corpus /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt @ Sun Jan  4 18:52:29 CST 2015
(1.3) numberizing corpus /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt @ Sun Jan  4 18:52:29 CST 2015
(2) running giza @ Sun Jan  4 18:52:29 CST 2015
(2.1a) running snt2cooc mod-anc @ Sun Jan  4 18:52:29 CST 2015

Executing: mkdir -p /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc
Executing: /Users/plum/Developer/NLP/working/../mosesdecoder/tools/snt2cooc.out /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt > /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.cooc
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/snt2cooc.out /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt > /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.cooc
line 1000
line 2000
END.
(2.1b) running giza mod-anc @ Sun Jan  4 18:52:29 CST 2015
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/GIZA++  -CoocurrenceFile /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.cooc -c /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt -hmmiterations 0 -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc -onlyaldumps 1 -p0 0.999 -s /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb -t /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb
Executing: /Users/plum/Developer/NLP/working/../mosesdecoder/tools/GIZA++  -CoocurrenceFile /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.cooc -c /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt -hmmiterations 0 -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc -onlyaldumps 1 -p0 0.999 -s /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb -t /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/GIZA++  -CoocurrenceFile /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.cooc -c /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt -hmmiterations 0 -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc -onlyaldumps 1 -p0 0.999 -s /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb -t /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb
ERROR: parameter 'coocurrencefile' does not exist.
WARNING: ignoring unrecognized option:  -CoocurrenceFile
ERROR: parameter 'usersplumdevelopernlpworkingtestancgizamodancmodanccooc' does not exist.
WARNING: ignoring unrecognized option:  /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.cooc
Parameter 'c' changed from '' to '/Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt'
Parameter 'hmmiterations' changed from '5' to '0'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '115-01-04.185229.plum' to '/Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb'
Parameter 't' changed from '' to '/Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 0  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 115-01-04.185229.plum.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb  (source vocabulary file name)
t = /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 0  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 115-01-04.185229.plum.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb  (source vocabulary file name)
t = /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb
Reading vocabulary file from:/Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb
Source vocabulary list has 4293 unique tokens 
Target vocabulary list has 4663 unique tokens 
Calculating vocabulary frequencies from corpus /Users/plum/Developer/NLP/working/test_anc/corpus/mod-anc-int-train.snt
Reading more sentence pairs into memory ... 
WARNING: The following sentence pair has source/target sentence length ration more than
the maximum allowed limit for a source word fertility
 source length = 1 target length = 10 ratio 10 ferility limit : 9
Shortening sentence 
Sent No: 285 , No. Occurrences: 1
0 2216 
282 1434 164 3 2945 3738 957 43 289 4068 
WARNING: The following sentence pair has source/target sentence length ration more than
the maximum allowed limit for a source word fertility
 source length = 1 target length = 12 ratio 12 ferility limit : 9
Shortening sentence 
Sent No: 1902 , No. Occurrences: 1
0 3166 
592 2158 92 49 162 512 3809 43 49 1362 98 145 
Corpus fits in memory, corpus has: 2564 sentence pairs.
 Train total # sentence pairs (weighted): 2564
Size of source portion of the training corpus: 8709 tokens
Size of the target portion of the training corpus: 15408 tokens 
In source portion of the training corpus, only 4291 unique tokens appeared
In target portion of the training corpus, only 4655 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 15408/(11273-2564)== 1.7692
==========================================================
Model1 Training Started at: Sun Jan  4 18:52:29 2015

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 12.8701 PERPLEXITY 7486.58
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 15.082 PERPLEXITY 34684.7
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 4.65699 PERPLEXITY 25.2286
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 5.73424 PERPLEXITY 53.2327
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.41426 PERPLEXITY 21.3219
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.36114 PERPLEXITY 41.1021
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.3084 PERPLEXITY 19.8134
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.15091 PERPLEXITY 35.5287
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.26329 PERPLEXITY 19.2035
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.04387 PERPLEXITY 32.988
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
==========================================================

Transfer started at: Sun Jan  4 18:52:30 2015

Transfering Model2 --> Model3 (i.e. estimating initial parameters of Model3 from Model2 tables)
Normalizing t, a, d, n count tables now ... A/D table contains 1799 parameters.
A/D table contains 2256 parameters.
NTable contains 42930 parameter.

Transfer: TRAIN CROSS-ENTROPY 25.2827 PERPLEXITY 4.08189e+07

Transfer took: 0 seconds

Transfer Finished at: Sun Jan  4 18:52:30 2015

==========================================================
Read classes: #words: 4292  #classes: 51
Read classes: #words: 4662  #classes: 51
Read classes: #words: 4292  #classes: 51
Read classes: #words: 4662  #classes: 51

==========================================================
Starting 3333444:  Viterbi Training
 3333444 Training Started at: Sun Jan  4 18:52:30 2015


---------------------
Model3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 37.064 #alsophisticatedcountcollection: 0 #hcsteps: 2.09165
#peggingImprovements: 0
A/D table contains 1799 parameters.
A/D table contains 2488 parameters.
NTable contains 42930 parameter.
p0_count is 13202.9 and p1 is 1102.53; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.637 PERPLEXITY 24.8814
Model3: (1) TRAIN VITERBI CROSS-ENTROPY 4.938 PERPLEXITY 30.6539

Model3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 37.2044 #alsophisticatedcountcollection: 0 #hcsteps: 1.45593
#peggingImprovements: 0
A/D table contains 1799 parameters.
A/D table contains 2488 parameters.
NTable contains 42930 parameter.
p0_count is 14061.5 and p1 is 673.274; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.35985 PERPLEXITY 20.5327
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.49844 PERPLEXITY 22.6029

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 37.2285 #alsophisticatedcountcollection: 0 #hcsteps: 1.3869
#peggingImprovements: 0
A/D table contains 1799 parameters.
A/D table contains 2454 parameters.
NTable contains 42930 parameter.
p0_count is 14662.1 and p1 is 372.975; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.12709 PERPLEXITY 17.4735
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 4.21791 PERPLEXITY 18.6088

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 37.2516 #alsophisticatedcountcollection: 10.0675 #hcsteps: 1.28237
#peggingImprovements: 0
D4 table contains 475629 parameters.
A/D table contains 1799 parameters.
A/D table contains 2454 parameters.
NTable contains 42930 parameter.
p0_count is 14926.3 and p1 is 240.828; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 4.01031 PERPLEXITY 16.1147
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 4.0748 PERPLEXITY 16.8514

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 37.2391 #alsophisticatedcountcollection: 6.18253 #hcsteps: 1.24649
#peggingImprovements: 0
D4 table contains 475629 parameters.
A/D table contains 1799 parameters.
A/D table contains 2454 parameters.
NTable contains 42930 parameter.
p0_count is 14952.5 and p1 is 227.728; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.91725 PERPLEXITY 15.1081
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.95884 PERPLEXITY 15.5499

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 37.2204 #alsophisticatedcountcollection: 4.30109 #hcsteps: 1.19891
#peggingImprovements: 0
D4 table contains 475629 parameters.
A/D table contains 1799 parameters.
A/D table contains 2454 parameters.
NTable contains 42930 parameter.
p0_count is 15005.2 and p1 is 201.424; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.81299 PERPLEXITY 14.0548
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.84468 PERPLEXITY 14.3669

Model4 Viterbi Iteration : 6 took: 1 seconds
3333444 Training Finished at: Sun Jan  4 18:52:31 2015


Entire Viterbi 3333444 Training took: 1 seconds
==========================================================

Entire Training took: 2 seconds
Program Finished at: Sun Jan  4 18:52:31 2015

==========================================================
Executing: rm -f /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.A3.final.gz
Executing: gzip /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.A3.final
(2.1a) running snt2cooc anc-mod @ Sun Jan  4 18:52:31 CST 2015

Executing: mkdir -p /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod
Executing: /Users/plum/Developer/NLP/working/../mosesdecoder/tools/snt2cooc.out /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt > /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.cooc
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/snt2cooc.out /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt > /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.cooc
line 1000
line 2000
END.
(2.1b) running giza anc-mod @ Sun Jan  4 18:52:31 CST 2015
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/GIZA++  -CoocurrenceFile /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.cooc -c /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt -hmmiterations 0 -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod -onlyaldumps 1 -p0 0.999 -s /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb -t /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb
Executing: /Users/plum/Developer/NLP/working/../mosesdecoder/tools/GIZA++  -CoocurrenceFile /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.cooc -c /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt -hmmiterations 0 -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod -onlyaldumps 1 -p0 0.999 -s /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb -t /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb
/Users/plum/Developer/NLP/working/../mosesdecoder/tools/GIZA++  -CoocurrenceFile /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.cooc -c /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt -hmmiterations 0 -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod -onlyaldumps 1 -p0 0.999 -s /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb -t /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb
ERROR: parameter 'coocurrencefile' does not exist.
WARNING: ignoring unrecognized option:  -CoocurrenceFile
ERROR: parameter 'usersplumdevelopernlpworkingtestancgizaancmodancmodcooc' does not exist.
WARNING: ignoring unrecognized option:  /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.cooc
Parameter 'c' changed from '' to '/Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt'
Parameter 'hmmiterations' changed from '5' to '0'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '115-01-04.185231.plum' to '/Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb'
Parameter 't' changed from '' to '/Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 0  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 115-01-04.185231.plum.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb  (source vocabulary file name)
t = /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 0  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 115-01-04.185231.plum.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb  (source vocabulary file name)
t = /Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/Users/plum/Developer/NLP/working/test_anc/corpus/mod.vcb
Reading vocabulary file from:/Users/plum/Developer/NLP/working/test_anc/corpus/anc.vcb
Source vocabulary list has 4663 unique tokens 
Target vocabulary list has 4293 unique tokens 
Calculating vocabulary frequencies from corpus /Users/plum/Developer/NLP/working/test_anc/corpus/anc-mod-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 2564 sentence pairs.
 Train total # sentence pairs (weighted): 2564
Size of source portion of the training corpus: 15428 tokens
Size of the target portion of the training corpus: 8709 tokens 
In source portion of the training corpus, only 4661 unique tokens appeared
In target portion of the training corpus, only 4290 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 8709/(17992-2564)== 0.564493
==========================================================
Model1 Training Started at: Sun Jan  4 18:52:31 2015

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.2088 PERPLEXITY 9467.5
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 16.0934 PERPLEXITY 69916.9
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 4.98474 PERPLEXITY 31.6634
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.287 PERPLEXITY 78.0866
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.70032 PERPLEXITY 25.9978
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.89024 PERPLEXITY 59.3114
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.57775 PERPLEXITY 23.8803
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.63341 PERPLEXITY 49.6393
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.52191 PERPLEXITY 22.9736
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.49344 PERPLEXITY 45.0496
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
==========================================================

Transfer started at: Sun Jan  4 18:52:31 2015

Transfering Model2 --> Model3 (i.e. estimating initial parameters of Model3 from Model2 tables)
Normalizing t, a, d, n count tables now ... A/D table contains 2436 parameters.
A/D table contains 1590 parameters.
NTable contains 46630 parameter.

Transfer: TRAIN CROSS-ENTROPY 24.8622 PERPLEXITY 3.04976e+07

Transfer took: 0 seconds

Transfer Finished at: Sun Jan  4 18:52:31 2015

==========================================================
Read classes: #words: 4662  #classes: 51
Read classes: #words: 4292  #classes: 51
Read classes: #words: 4662  #classes: 51
Read classes: #words: 4292  #classes: 51

==========================================================
Starting 3333444:  Viterbi Training
 3333444 Training Started at: Sun Jan  4 18:52:31 2015


---------------------
Model3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 29.1197 #alsophisticatedcountcollection: 0 #hcsteps: 1.69462
#peggingImprovements: 0
A/D table contains 2436 parameters.
A/D table contains 1680 parameters.
NTable contains 46630 parameter.
p0_count is 7956.03 and p1 is 376.486; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.43488 PERPLEXITY 21.6287
Model3: (1) TRAIN VITERBI CROSS-ENTROPY 4.88631 PERPLEXITY 29.5751

Model3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 29.0839 #alsophisticatedcountcollection: 0 #hcsteps: 1.36622
#peggingImprovements: 0
A/D table contains 2436 parameters.
A/D table contains 1655 parameters.
NTable contains 46630 parameter.
p0_count is 8613.43 and p1 is 47.7848; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.88293 PERPLEXITY 14.7529
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.13278 PERPLEXITY 17.5425

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 29.1076 #alsophisticatedcountcollection: 0 #hcsteps: 1.33034
#peggingImprovements: 0
A/D table contains 2436 parameters.
A/D table contains 1655 parameters.
NTable contains 46630 parameter.
p0_count is 8669.64 and p1 is 19.6812; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.65214 PERPLEXITY 12.572
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.83539 PERPLEXITY 14.2748

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 29.1408 #alsophisticatedcountcollection: 8.7929 #hcsteps: 1.31825
#peggingImprovements: 0
D4 table contains 506282 parameters.
A/D table contains 2436 parameters.
A/D table contains 1643 parameters.
NTable contains 46630 parameter.
p0_count is 8673.8 and p1 is 17.5986; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.5379 PERPLEXITY 11.6149
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.68744 PERPLEXITY 12.8834

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 29.1704 #alsophisticatedcountcollection: 6.55304 #hcsteps: 1.32059
#peggingImprovements: 0
D4 table contains 506282 parameters.
A/D table contains 2436 parameters.
A/D table contains 1581 parameters.
NTable contains 46630 parameter.
p0_count is 8667.01 and p1 is 20.9941; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.43775 PERPLEXITY 10.8359
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.55348 PERPLEXITY 11.741

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 29.1958 #alsophisticatedcountcollection: 5.18331 #hcsteps: 1.29173
#peggingImprovements: 0
D4 table contains 506282 parameters.
A/D table contains 2436 parameters.
A/D table contains 1581 parameters.
NTable contains 46630 parameter.
p0_count is 8670.51 and p1 is 19.2471; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.3035 PERPLEXITY 9.87304
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.39882 PERPLEXITY 10.5474

Model4 Viterbi Iteration : 6 took: 1 seconds
3333444 Training Finished at: Sun Jan  4 18:52:32 2015


Entire Viterbi 3333444 Training took: 1 seconds
==========================================================

Entire Training took: 1 seconds
Program Finished at: Sun Jan  4 18:52:32 2015

==========================================================
Executing: rm -f /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.A3.final.gz
Executing: gzip /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.A3.final
(3) generate word alignment @ Sun Jan  4 18:52:32 CST 2015
Combining forward and inverted alignment from files:
  /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.A3.final.{bz2,gz}
  /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.A3.final.{bz2,gz}
Executing: mkdir -p /Users/plum/Developer/NLP/working/test_anc/model
Executing: /Users/plum/Developer/NLP/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /Users/plum/Developer/NLP/working/test_anc/giza.anc-mod/anc-mod.A3.final.gz" -i "gzip -cd /Users/plum/Developer/NLP/working/test_anc/giza.mod-anc/mod-anc.A3.final.gz" |/Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /Users/plum/Developer/NLP/working/test_anc/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
Sentence mismatch error! Line #285
Sentence mismatch error! Line #1902
skip=<0> counts=<2564>
(4) generate lexical translation table 0-0 @ Sun Jan  4 18:52:32 CST 2015
(/Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.mod,/Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.anc,/Users/plum/Developer/NLP/working/test_anc/model/lex)
!!!
Saved: /Users/plum/Developer/NLP/working/test_anc/model/lex.f2e and /Users/plum/Developer/NLP/working/test_anc/model/lex.e2f
FILE: /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.anc
FILE: /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.mod
FILE: /Users/plum/Developer/NLP/working/test_anc/model/aligned.grow-diag-final-and
(5) extract phrases @ Sun Jan  4 18:52:32 CST 2015
/Users/plum/Developer/NLP/mosesdecoder/scripts/generic/extract-parallel.perl 1 split "sort    " /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/extract /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.anc /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.mod /Users/plum/Developer/NLP/working/test_anc/model/aligned.grow-diag-final-and /Users/plum/Developer/NLP/working/test_anc/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /Users/plum/Developer/NLP/mosesdecoder/scripts/generic/extract-parallel.perl 1 split "sort    " /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/extract /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.anc /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.mod /Users/plum/Developer/NLP/working/test_anc/model/aligned.grow-diag-final-and /Users/plum/Developer/NLP/working/test_anc/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sun Jan  4 18:52:32 2015
Executing: ln -s /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.anc /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/target.0000000 
total=2564 line-per-split=2565 
Executing: ln -s /Users/plum/Developer/NLP/working/../corpus/corpus.tokenized.strip.anc-mod.mod /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/source.0000000 
Executing: ln -s /Users/plum/Developer/NLP/working/test_anc/model/aligned.grow-diag-final-and /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/align.0000000 
/Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/extract /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/target.0000000 /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/source.0000000 /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/align.0000000 /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/extract.0000000  7 orientation --model wbe-msd --GZOutput   --SentenceOffset 0 2>> /dev/stderr 
glueArg= 
PhraseExtract v1.4, written by Philipp Koehn
phrase extraction from an aligned parallel corpus
merging extract / extract.inv
gunzip -c /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/extract.0000000.gz  | LC_ALL=C sort     -T /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450 2>> /dev/stderr | gzip -c > /Users/plum/Developer/NLP/working/test_anc/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/extract.0000000.inv.gz  | LC_ALL=C sort     -T /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450 2>> /dev/stderr | gzip -c > /Users/plum/Developer/NLP/working/test_anc/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450/extract.0000000.o.gz  | LC_ALL=C sort     -T /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450 2>> /dev/stderr | gzip -c > /Users/plum/Developer/NLP/working/test_anc/model/extract.o.sorted.gz 2>> /dev/stderr 
rm -rf /Users/plum/Developer/NLP/working/test_anc/model/tmp.84450 
Finished Sun Jan  4 18:52:32 2015
(6) score phrases @ Sun Jan  4 18:52:32 CST 2015
(6.1)  creating table half /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.f2e @ Sun Jan  4 18:52:32 CST 2015
/Users/plum/Developer/NLP/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/score /Users/plum/Developer/NLP/working/test_anc/model/extract.sorted.gz /Users/plum/Developer/NLP/working/test_anc/model/lex.f2e /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.f2e.gz  0 
Executing: /Users/plum/Developer/NLP/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/score /Users/plum/Developer/NLP/working/test_anc/model/extract.sorted.gz /Users/plum/Developer/NLP/working/test_anc/model/lex.f2e /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.f2e.gz  0 
Started Sun Jan  4 18:52:32 2015
ln -s /Users/plum/Developer/NLP/working/test_anc/model/extract.sorted.gz /Users/plum/Developer/NLP/working/test_anc/model/tmp.84480/extract.0.gz 
/Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/score /Users/plum/Developer/NLP/working/test_anc/model/tmp.84480/extract.0.gz /Users/plum/Developer/NLP/working/test_anc/model/lex.f2e /Users/plum/Developer/NLP/working/test_anc/model/tmp.84480/phrase-table.half.0000000.gz  2>> /dev/stderr 
/Users/plum/Developer/NLP/working/test_anc/model/tmp.84480/run.0.shScore v2.1 -- scoring methods for extracted rules
Loading lexical translation table from /Users/plum/Developer/NLP/working/test_anc/model/lex.f2e
mv /Users/plum/Developer/NLP/working/test_anc/model/tmp.84480/phrase-table.half.0000000.gz /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.f2e.gzrm -rf /Users/plum/Developer/NLP/working/test_anc/model/tmp.84480 
Finished Sun Jan  4 18:52:33 2015
(6.3)  creating table half /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.e2f @ Sun Jan  4 18:52:33 CST 2015
/Users/plum/Developer/NLP/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/score /Users/plum/Developer/NLP/working/test_anc/model/extract.inv.sorted.gz /Users/plum/Developer/NLP/working/test_anc/model/lex.e2f /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /Users/plum/Developer/NLP/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/score /Users/plum/Developer/NLP/working/test_anc/model/extract.inv.sorted.gz /Users/plum/Developer/NLP/working/test_anc/model/lex.e2f /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.e2f.gz --Inverse 1 
Started Sun Jan  4 18:52:33 2015
ln -s /Users/plum/Developer/NLP/working/test_anc/model/extract.inv.sorted.gz /Users/plum/Developer/NLP/working/test_anc/model/tmp.84490/extract.0.gz 
/Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/score /Users/plum/Developer/NLP/working/test_anc/model/tmp.84490/extract.0.gz /Users/plum/Developer/NLP/working/test_anc/model/lex.e2f /Users/plum/Developer/NLP/working/test_anc/model/tmp.84490/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/Users/plum/Developer/NLP/working/test_anc/model/tmp.84490/run.0.shScore v2.1 -- scoring methods for extracted rules
using inverse mode
Loading lexical translation table from /Users/plum/Developer/NLP/working/test_anc/model/lex.e2f
gunzip -c /Users/plum/Developer/NLP/working/test_anc/model/tmp.84490/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /Users/plum/Developer/NLP/working/test_anc/model/tmp.84490  | gzip -c > /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /Users/plum/Developer/NLP/working/test_anc/model/tmp.84490 
Finished Sun Jan  4 18:52:33 2015
(6.6) consolidating the two halves @ Sun Jan  4 18:52:33 CST 2015
Executing: /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/consolidate /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.f2e.gz /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
Executing: rm -f /Users/plum/Developer/NLP/working/test_anc/model/phrase-table.half.*
(7) learn reordering model @ Sun Jan  4 18:52:34 CST 2015
(7.1) [no factors] learn reordering model @ Sun Jan  4 18:52:34 CST 2015
(7.2) building tables @ Sun Jan  4 18:52:34 CST 2015
Executing: /Users/plum/Developer/NLP/mosesdecoder/scripts/../bin/lexical-reordering-score /Users/plum/Developer/NLP/working/test_anc/model/extract.o.sorted.gz 0.5 /Users/plum/Developer/NLP/working/test_anc/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sun Jan  4 18:52:34 CST 2015
  no generation model requested, skipping step
(9) create moses.ini @ Sun Jan  4 18:52:34 CST 2015
Argument "KENLM lazyken=0" isn't numeric in numeric eq (==) at ../mosesdecoder/scripts/training/train-model.perl line 2127.
